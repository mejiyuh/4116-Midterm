# 4116-Midterm

The Midterm exam will be based on building a Generative Adversarial Network (GAN) and using it to generate high resolution images for the binary classification problem you have already implemented in Assignment 1.

Dataset: The dataset for this project is Kaggle Retinal OCT images in two versions (default and tiny). 

Introduction: Super Resolution GAN (SRGAN)  (https://arxiv.org/pdf/1609.04802) is a deep learning architecture that uses a combination of GANs and convolutional neural networks (CNNs) to generate high-resolution images from low-resolution images. The idea behind SRGAN is to train a generator network to create high-resolution images that are as close as possible to the real high-resolution images, and a discriminator network that is trained to distinguish between the generated high-resolution images and real high-resolution images. The training process involves feeding low-resolution images to the generator, which then generates a high-resolution image. The discriminator then evaluates the generated high-resolution image and provides feedback to the generator to improve the quality of the generated image. The generator and discriminator networks are trained iteratively until the generated images are of sufficient quality. Super Resolution GAN has many practical applications, such as in medical imaging, satellite imagery, and video processing. It can help to enhance the quality of low-resolution images, making them more useful for analysis and decision-making.

<img src="https://github.com/user-attachments/assets/b958dbc8-9510-4094-97a4-49b0b5570dda" 
     style="max-width: 100%; height: auto;" />


Steps:

Train a binary classifier (called A) on the dataset using transfer learning (exactly like Assignment 1). The images should be downscaled to 128x128
Next, train the SRGAN to generate 128x128 images. Each image of the training is downscaled to 32x32.
Show some examples of scaled images in JNB
Utilize the images generated by SRGAN in order to train a new model (called B)
Train the SRGAN for at least 150 epochs
Divide the dataset into 70% training and 30% testing
Apply normalization and image transformation, and demonstrate some of the transformed samples
Compare the performance of both models using different metrics such as F1, Accuracy, AUC
Since there is limited time to use Google Colab GPU every 12 hours, save your models after each n epoch
