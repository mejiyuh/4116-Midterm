{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mejiyuh/4116-Midterm/blob/main/Github_4116Midterm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "#mount google drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Paths for the datasets\n",
        "train_data_dir = '/content/drive/My Drive/Data'  # default dataset\n",
        "tiny_data_dir = '/content/drive/My Drive/Data_Small/train'  # tiny dataset train\n",
        "test_tiny_data_dir = '/content/drive/My Drive/Data_Small/test'  # tiny dataset test\n",
        "\n",
        "#copying datasets to local storage for faster access (hoping for)\n",
        "shutil.copytree(train_data_dir, '/content/Data')\n",
        "shutil.copytree(tiny_data_dir, '/content/Data_Small/train')\n",
        "shutil.copytree(test_tiny_data_dir, '/content/Data_Small/test')\n",
        "\n",
        "train_data_dir = '/content/Data/Data/train'\n",
        "test_data_dir = '/content/Data/Data/test'\n",
        "train_tiny_data_dir = '/content/Data_Small/train'\n",
        "test_tiny_data_dir = '/content/Data_Small/test'\n",
        "\n",
        "import os\n",
        "\n",
        "print(\"Train Data Subdirectories:\")\n",
        "print(os.listdir(train_data_dir))\n",
        "print(\"Test Data Subdirectories:\")\n",
        "print(os.listdir(test_data_dir))\n",
        "\n",
        "print(\"Tiny train Data Subdirectories:\")\n",
        "print(os.listdir(tiny_data_dir))\n",
        "print(\"Tiny test Data Subdirectories:\")\n",
        "print(os.listdir(test_tiny_data_dir))\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, BatchNormalization\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
        "from sklearn.utils import class_weight\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "IMG_SIZE = (128, 128)\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "\n",
        "def preprocess_and_convert(image):\n",
        "    #normalization\n",
        "    image = tf.cast(image, tf.float32) / 255.0\n",
        "    mean = tf.constant([0.485, 0.456, 0.406], dtype=tf.float32)\n",
        "    std = tf.constant([0.229, 0.224, 0.225], dtype=tf.float32)\n",
        "    image = (image - mean) / std\n",
        "\n",
        "    #rgb check\n",
        "    if image.shape[-1] == 1:\n",
        "        image = tf.image.grayscale_to_rgb(image)\n",
        "\n",
        "    return image\n",
        "\n",
        "#data generators for training/validation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_and_convert,\n",
        "    rotation_range=15,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    shear_range=0.1,\n",
        "    validation_split=0.3\n",
        ")\n",
        "\n",
        "test_datagen = ImageDataGenerator(\n",
        "    preprocessing_function=preprocess_and_convert\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory=train_data_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='training',\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "    directory=train_data_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    subset='validation',\n",
        "    shuffle=False,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size=IMG_SIZE,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='binary',\n",
        "    shuffle=False\n",
        ")\n",
        "#convert to a tf.data dataset (in hopes of improving runtime)\n",
        "def generator_to_dataset(generator):\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        lambda: generator,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(None, *IMG_SIZE, 3), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
        "        )\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "train_dataset = generator_to_dataset(train_generator).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
        "validation_dataset = generator_to_dataset(validation_generator).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
        "test_dataset = generator_to_dataset(test_generator).cache().prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "#class weights for imbalanced data\n",
        "class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(train_generator.classes), y=train_generator.classes)\n",
        "class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "#load model (used VGG19 instead of ResNet50)\n",
        "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "for layer in base_model.layers[:-4]:  #freeze layers in base model\n",
        "    layer.trainable = False\n",
        "\n",
        "#custom layers for classification.\n",
        "x = GlobalAveragePooling2D()(base_model.output)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = BatchNormalization()(x)\n",
        "output = Dense(1, activation='sigmoid')(x)\n",
        "#final model\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "#compile the model\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#callbacks-added lr_scheduler\n",
        "checkpoint = ModelCheckpoint('best_model_vgg19.keras', monitor='val_loss', save_best_only=True, mode='min')\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-5)\n",
        "\n",
        "\n",
        "#train the model\n",
        "#Google Colab GPU compute units :/\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[lr_scheduler, early_stopping, checkpoint],\n",
        "    class_weight=class_weight_dict\n",
        ")\n",
        "\n",
        "#test set evaluation\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(f\"Test Loss: {test_loss}\")\n",
        "print(f\"Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "#plot training/validation accuracy\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'], label='train_accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "#plot training/validation loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'], label='train_loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "#confusion Matrix\n",
        "predictions = model.predict(test_generator)\n",
        "predicted_classes = (predictions > 0.5).astype(int)\n",
        "true_classes = test_generator.classes\n",
        "cm = confusion_matrix(true_classes, predicted_classes)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Purples',\n",
        "            xticklabels=list(test_generator.class_indices.keys()),\n",
        "            yticklabels=list(test_generator.class_indices.keys()))\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5Dx4JVZ-F3zV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c5d2102-df31-4277-9511-b12f6df78afc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Train Data Subdirectories:\n",
            "['DME', 'DRUSEN']\n",
            "Test Data Subdirectories:\n",
            "['DME', 'DRUSEN']\n",
            "Tiny train Data Subdirectories:\n",
            "['DME', 'DRUSEN']\n",
            "Tiny test Data Subdirectories:\n",
            "['DME', 'DRUSEN']\n",
            "Found 13386 images belonging to 2 classes.\n",
            "Found 5736 images belonging to 2 classes.\n",
            "Found 930 images belonging to 2 classes.\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg19/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m80134624/80134624\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m166s\u001b[0m 698ms/step - accuracy: 0.8425 - loss: 0.3431 - val_accuracy: 0.7136 - val_loss: 1.1338 - learning_rate: 0.0010\n",
            "Epoch 2/10\n",
            "\u001b[1m160/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 461ms/step - accuracy: 0.9271 - loss: 0.1746"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation metrics\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "precision = precision_score(true_classes, predicted_classes)\n",
        "recall = recall_score(true_classes, predicted_classes)\n",
        "f1 = f1_score(true_classes, predicted_classes)\n",
        "\n",
        "print(f'Precision: {precision}')\n",
        "print(f'Recall: {recall}')\n",
        "print(f'F1 Score: {f1}')\n"
      ],
      "metadata": {
        "id": "GK3F-kQ6YIUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "#predictions for the test set\n",
        "test_generator.reset()\n",
        "y_pred = model.predict(test_generator, steps=len(test_generator))\n",
        "y_pred = (y_pred > 0.5).astype(int)\n",
        "y_true = test_generator.classes\n",
        "#class distribution in the test set\n",
        "print(\"\\nClass distribution in the test set:\")\n",
        "for class_name, class_index in test_generator.class_indices.items():\n",
        "    class_count = np.sum(true_classes == class_index)\n",
        "    print(f\"{class_name}: {class_count}\")\n",
        "\n",
        "#classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(true_classes, predicted_classes, target_names=list(test_generator.class_indices.keys())))"
      ],
      "metadata": {
        "id": "ygH29pCwYMXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#precision-recall curve for the positive class\n",
        "precision, recall, _ = precision_recall_curve(y_true, y_pred[:, 0])\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "#plot curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='blue', label=f'Precision-Recall AUC = {pr_auc:.2f}')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.legend(loc='best')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nVmNavckYOUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#predicted probabilities for the positive class\n",
        "y_pred_prob = y_pred[:, 0]  # Single output, class 1 probabilities\n",
        "\n",
        "#calculations for ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_pred_prob)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "#plot curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.xlabel('False Positive Rate (FPR)')\n",
        "plt.ylabel('True Positive Rate (TPR)')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0RQZNSiGYQ0M"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}